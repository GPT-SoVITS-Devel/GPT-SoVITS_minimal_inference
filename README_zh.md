<div align="center">

# âš¡ GPT-SoVITS Minimal Inference
**High-Performance | Production-Ready | Zero-Copy Pipeline**

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE) [![Python](https://img.shields.io/badge/python-3.9+-green.svg)](https://www.python.org/) [![GPU](https://img.shields.io/badge/CUDA-11.8+-orange.svg)](https://developer.nvidia.com/cuda-zone) [![ONNX](https://img.shields.io/badge/ONNX-Optimized-brightgreen.svg)](https://onnxruntime.ai/) [![TensorRT](https://img.shields.io/badge/TensorRT-Enabled-76B900.svg)](https://developer.nvidia.com/tensorrt) 

[ç®€ä½“ä¸­æ–‡](./README_zh.md) | [English](./README.md)

**â€œä¸ä»…æ˜¯ä»£ç é‡æ„ï¼Œæ›´æ˜¯å¯¹ GPT-SoVITS æ½œåŠ›çš„æ·±åº¦å‹æ¦¨ã€‚â€**

---
**Engineered for Speed**: A completely refactored inference engine for GPT-SoVITS, featuring ONNX/TensorRT support, KV-Cache optimization, and zero-copy streaming.
</div>

---

## ğŸŒŸ æ ¸å¿ƒæ„¿æ™¯ (Core Vision)

åœ¨ä¸ç ´ååŸæ¨¡å‹ç²¾åº¦ã€ä¸é‡æ–°è®­ç»ƒçš„å‰æä¸‹ï¼Œé€šè¿‡åº•å±‚ç®—å­é‡å†™ä¸æ¶æ„è§£è€¦ï¼Œå½»åº•è§£å†³ GPT-SoVITS åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚

æˆ‘ä»¬è¿½æ±‚çš„æ˜¯ï¼š**å¿«é€Ÿ (Fast)**ã€**è½»é‡ (Lightweight)**ã€**é«˜å…¼å®¹ (Compatible)**ã€**å¯ç§»æ¤ (Portable)**ã€‚

## ğŸš€ æ€§èƒ½å¯¹æ¯” (Performance Benchmarks)

*æµ‹è¯•ç¯å¢ƒ: I7 12700 | RTX 2080TI (22G) | CUDA 12.9 | FP16 ç²¾åº¦*

| Metric                      | Native PyTorch | ONNX (fp16) | ONNX Stream | TensorRT (FP16)      |
|:----------------------------|:---------------|:------------|:------------|:---------------------|
| **First Token Latency (â†“)** | 2.524 s        | 1.983 s     | **1.000 s** | 2.022 s              |
| **Inference Speed (â†‘)**     | 144.8 tok/s    | 172.4 tok/s | 167.5 tok/s | **291.6 tok/s** (ğŸ¤¯) |
| **RTF (â†“)**                 | 0.3434         | 0.3325      | 0.3100      | **0.2096**           |
| **VRAM Usage (â†“)**          | 2.8 G          | 3.9 G       | 4.5 G       | 4.8 G                |

---

## ğŸ› ï¸ æ·±åº¦åˆ†æï¼šä¸ºä½•é‡æ„ï¼Ÿ (The "Why")

### 1. æ¶ˆé™¤åŠ¨æ€å›¾ä¸ Python å¼€é”€
åŸç‰ˆ `GPT-SoVITS` åŸºäº PyTorch åŠ¨æ€å›¾ï¼Œåœ¨ AR è§£ç é˜¶æ®µï¼Œæ¯ç”Ÿæˆä¸€ä¸ª Token éƒ½ä¼šäº§ç”Ÿæ˜¾è‘—çš„ Python è§£é‡Šå™¨è°ƒåº¦å¼€é”€ã€‚åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹ï¼Œè¿™ç§çº¿æ€§ç´¯ç§¯çš„å»¶è¿Ÿæ˜¯ç”Ÿäº§ç¯å¢ƒçš„å™©æ¢¦ã€‚

### 2. æè‡´çš„æ˜¾å­˜ç®¡ç†ä¼˜åŒ–
*   **KV-Cache é¢„åˆ†é…**ï¼šè§„é¿äº† ONNX å¯¼å‡ºåå¸¸è§çš„ `torch.cat` å¯¼è‡´çš„ç©ºè½¬ä¸é¢‘ç¹å†…å­˜æ‹·è´ã€‚
*   **é™æ€ç»´åº¦å¯¹é½**ï¼šé’ˆå¯¹ TensorRT è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿é™æ€æ‰§è¡Œè®¡åˆ’çš„ç¨³å®šæ€§ï¼Œè§„é¿åŠ¨æ€ Shape å¯¼è‡´çš„ Re-build é—®é¢˜ã€‚

---

## ğŸ’ æ ¸å¿ƒé»‘ç§‘æŠ€ (Core Optimizations)

### 1. æ‰‹æœ¯åˆ€çº§ç®—å­é‡å†™
æˆ‘ä»¬å°† GPT æ¨¡å‹æ‹†è§£ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„è®¡ç®—å›¾ï¼š
*   **`GPTEncoder` (Context Phase)**: ä¸€æ¬¡æ€§å¤„ç† Prompt ä¸ BERT ç‰¹å¾ã€‚
*   **`GPTStep` (Decoding Phase)**: æ‰§è¡Œ $O(1)$ å¤æ‚åº¦çš„å•æ­¥è§£ç ï¼Œå¹¶å°† **Top-K Sampling** ä¸‹æ²‰è‡³ ONNX å›¾å†…éƒ¨ï¼Œå·¨é‡å‡å°‘ GPU->CPU æ•°æ®ä¼ è¾“ã€‚

### 2. å…¨é“¾è·¯ Zero-Copy Pipeline
åˆ©ç”¨ ONNX Runtime çš„ `IOBinding` æŠ€æœ¯ï¼š
*   **æ˜¾å­˜é©»ç•™**ï¼šè¾“å…¥è¾“å‡ºç›´æ¥ç»‘å®šæ˜¾å­˜åœ°å€ï¼Œä¸Šä¸€è½®çš„ `new_k_cache` ç›´æ¥ä½œä¸ºä¸‹è½®è¾“å…¥ï¼Œå½»åº•æ¶ˆé™¤ PCIe å¸¦å®½ç“¶é¢ˆã€‚

### 3. æµå¼æ¨ç†å»ä¼ªå½± (Artifact-Free)
ç‹¬åˆ› **Lookahead + History Window** æœºåˆ¶ï¼š
*   åœ¨ Chunk è¾¹ç•Œè¿›è¡Œçº¿æ€§åŠ æƒèåˆ (Cross-Fade)ï¼Œå½»åº•æ¶ˆé™¤ä¼ ç»Ÿæµå¼æ¨ç†å¸¸è§çš„â€œå’”å“’â€å£°ã€‚

---

## ğŸ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. å¯¼å‡ºæ¨¡å‹ (Export)
```bash
python export_onnx.py \
    --gpt_path "weights/gpt.ckpt" \
    --sovits_path "weights/sovits.pth" \
    --output_dir "onnx_export/optimized" \
    --max_len 1000
```

### 2. ç²¾åº¦è½¬æ¢ (Optional)
```bash
python onnx_to_fp16.py \
    --input_dir "onnx_export/optimized" \
    --output_dir "onnx_export/optimized_fp16"
```

### 3. å¼€å¯æé€Ÿæ¨ç† (Run)
```bash
# çº¯æµå¼æ¨ç†
python run_onnx_streaming_inference.py --onnx_dir "onnx_export/optimized_fp16" --text "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€æ®µæé€Ÿæµ‹è¯•ã€‚"

# å¯åŠ¨å…¨ç‰¹æ€§ WebUI
python run_optimized_inference.py --webui
```

### ONNX ä¼˜åŒ–FP16

```bash
# onnxä¸‹å¯¹fp16çš„åŠ é€Ÿä¸å¤ªæ˜æ˜¾,ä½†æ˜¯å¯¹æ˜¾å­˜ä¼˜åŒ–æ‹¥æœ‰æå¤§å¥½å¤„
python onnx_to_fp16.py --input_dir "onnx_export/optimized" \
  --output_dir "onnx_export/optimized_fp16"
```

### å¯¼å‡ºtrt

> ç¼–è¯‘trtæ—¶é—´è¾ƒä¹…æ˜¯æ­£å¸¸æƒ…å†µ,æ¯å°æœºå™¨åœ¨cuda/trtç‰ˆæœ¬ä¸ä¸€è‡´æ—¶ä¸€å®šè¦é‡æ–°ç¼–è¯‘!!!

```bash
# linux
onnx2trt.sh <onnx_input_dir> <output_dir>
# windows
onnx2trt.bat <onnx_input_dir> <output_dir>
```

---

## ğŸ—ºï¸ è·¯çº¿å›¾ (Roadmap)

- [x] **V2 / V2ProPlus** å®Œæ•´æ”¯æŒ
- [x] **TensorRT** é™æ€å¼•æ“åŠ é€Ÿ
- [x] **Zero-Copy** IOBinding ä¼˜åŒ–
- [ ] **Multi-Language Binding**:
    - [ ] C++ SDK (ç ”å‘ä¸­)
    - [ ] Rust / Golang / Android Wrapper
- [ ] **V3 / V4** æ¨¡å‹å¿«é€Ÿé€‚é…
- [ ] **Docker** ä¸€é”®éƒ¨ç½²é•œåƒ

---

## ğŸ¤ è‡´è°¢

æ„Ÿè°¢ [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) å›¢é˜Ÿæä¾›çš„å“è¶Šåº•åº§ã€‚æœ¬é¡¹ç›®è‡´åŠ›äºåœ¨å·¥ç¨‹åŒ–é“è·¯ä¸Šæ›´è¿›ä¸€æ­¥ã€‚

**å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç‚¹ä¸€ä¸ª â­ï¼Œè¿™æ˜¯æˆ‘ä»¬æŒç»­ä¼˜åŒ–çš„åŠ¨åŠ›ï¼ğŸ¤—**
